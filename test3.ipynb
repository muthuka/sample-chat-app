{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Demo\n",
    "Using RAG to power up an LLM. We will use Langchain for our example. Langchain framework makes build LLM apps super easy.\n",
    "\n",
    "![./flow.png](./flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Updated for LangChain 0.3.x (Pinecone Version)\n",
    "\n",
    "This notebook demonstrates RAG with Pinecone hosted vector store using latest APIs:\n",
    "\n",
    "- **Document Loaders**: `langchain-community` package\n",
    "- **Text Splitters**: `langchain-text-splitters` for better modularity\n",
    "- **LLMs**: `langchain-openai` for all OpenAI integrations\n",
    "- **Vector Store**: Pinecone with `langchain-pinecone` integration\n",
    "- **Chains**: LCEL approach for production-ready streaming\n",
    "- **Method Calls**: Updated to `.invoke()` pattern\n",
    "\n",
    "Includes both traditional and modern implementation patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! pip3 install -q langchain chromadb pypdf openai faiss-cpu \\\n",
    "        langchain-openai langchain_pinecone \\\n",
    "        pinecone-client ipywidgets langchain-community \\\n",
    "        langchain-text-splitters langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "### Step 1: Extract\n",
    "Load a document and extract the contents. For our example, I added a sample PDF from my article in docs folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 7 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs:  19\n",
      "First doc:  Generative AI project — Part 1Muthu Arumugam\n",
      "This article will help you understand how you can get i\n",
      "Last doc:  The model couldn’t come up with 6-word summary for the same sample.\n",
      "The sentence prematurely ended. \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load PDF\n",
    "loaders = [\n",
    "    PyPDFLoader(\"docs/GenAI-Part1.pdf\"),\n",
    "    PyPDFLoader(\"docs/GenAI-Part2.pdf\")\n",
    "]\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "# Look into the doc\n",
    "print(\"Total docs: \", len(docs))\n",
    "print(\"First doc: \", docs[0].page_content[0:100])\n",
    "print(\"Last doc: \", docs[-1].page_content[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split\n",
    "Now split the document contents into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents on Charter Splitter:  25\n",
      "Total documents on Recursive Charter Splitter:  25\n",
      "Total documents on Token Splitter:  44\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Character Splitter\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    separator = '\\n',\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "c_docs = c_splitter.split_documents(docs)\n",
    "print(\"Total documents on Charter Splitter: \", len(c_docs))\n",
    "\n",
    "# Recursive Character Splitter\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "r_docs = r_splitter.split_documents(docs)\n",
    "print(\"Total documents on Recursive Charter Splitter: \", len(r_docs))\n",
    "\n",
    "# Token Splitter\n",
    "t_splitter = TokenTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "t_docs = t_splitter.split_documents(docs)\n",
    "print(\"Total documents on Token Splitter: \", len(t_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Vector Store\n",
    "Let's take our splits and embed them and then store them into a vector store. We will use [Pinecone](https://python.langchain.com/docs/integrations/vectorstores/pinecone) which is hosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "# initialize pinecone\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "\n",
    "index_name = \"demo1\"\n",
    "# First, check if our index already exists. If it doesn't, we create it\n",
    "if index_name not in pc.list_indexes():\n",
    "    # we create a new index\n",
    "    pc.create_index(\n",
    "        name=index_name, \n",
    "        metric=\"cosine\", \n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "        dimension=1536)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the pages of our PDF into Vector Store with Embeddings using OpenAI Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3046ba17-5bfd-4504-971e-4f56e1829d46',\n",
       " 'fcd10af2-a1fe-4130-841b-e37ccfbec1a6',\n",
       " 'b9ce9fdb-b1f6-4129-949b-736686a55978',\n",
       " 'ecca860f-2720-4368-88bc-92b8a93efeda',\n",
       " 'adb03900-b106-4fda-b9ac-f6529624df9a',\n",
       " 'dbf71ab2-1880-41e3-ac75-76a9643a3db4',\n",
       " 'bd1f30a7-7f0b-4339-8950-20fc35b3d95c',\n",
       " '6709a07e-cd13-473c-85a7-92cc6a4767c0',\n",
       " '2e653b3a-8d43-451c-ae59-ad111cbfd39e',\n",
       " '3bcc35e0-d664-4df7-aea9-ef53164dbb74',\n",
       " '8bd6136c-b754-4d33-b3b1-b8ad56f61983',\n",
       " 'f4f992e3-5611-4bac-a3ed-b411cd191a0f',\n",
       " '7eeabacf-882b-400d-aff8-8382b122e607',\n",
       " '412a50c9-d4a9-48d2-a716-2b9e37bc3a5c',\n",
       " 'edc96b70-744d-4685-ba56-d796227b341c',\n",
       " '39c6ffcd-9332-4c20-970e-0e2350e56870',\n",
       " 'e8f627de-8c31-4254-924f-3a39a3c24a30',\n",
       " 'e56008cf-b027-4e7e-8f27-54fcb28026e9',\n",
       " 'e4f6e457-1056-4e80-a79f-cf93abcdaf30',\n",
       " '81a6ad5e-1b32-4a4c-85a2-25253a09c544',\n",
       " '35a66862-a013-41ee-bdbc-ea812789e444',\n",
       " 'f78b9cdb-6840-4a99-a226-d598bea1f71d',\n",
       " '62e248c1-cfd6-4388-9e76-24be0a31a4d3',\n",
       " 'c289207b-0a08-4cf3-b18e-08e701cc8f54',\n",
       " 'be2e49e6-21fc-4296-9ac7-767c514f39d7']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\n",
    "index = pc.Index(index_name)\n",
    "vector_store = PineconeVectorStore(index=index, embedding=OpenAIEmbeddings())\n",
    "vector_store.add_documents(documents=c_docs)\n",
    "\n",
    "# print(\"Total documents on Vector Store: \", len(vectordb.documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Retrieve\n",
    "Let's retrieve with different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Search\n",
      "Page #3.0  : Disclaimer: This is not generated by an AI bot. Also, a lot of these were\n",
      "learned through the DeepLe\n",
      "Page #14.0  : The model couldn’t come up with 6-word summary for the same sample.\n",
      "The sentence prematurely ended. \n",
      "Page #0.0  : Generative AI project — Part 1Muthu Arumugam\n",
      "This article will help you understand how you can get i\n",
      "Page #0.0  : Generative AI Project —  Part 2Muthu Arumugam\n",
      "To understand how AI projects work, see Part 1.\n",
      "This p\n",
      "\n",
      "MMR Search\n",
      "Page #3.0  : Disclaimer: This is not generated by an AI bot. Also, a lot of these were\n",
      "learned through the DeepLe\n",
      "Page #0.0  : Generative AI project — Part 1Muthu Arumugam\n",
      "This article will help you understand how you can get i\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is the author?\"\n",
    "\n",
    "# Using Similarity Search\n",
    "print(\"\\nSimilarity Search\")\n",
    "docs = vector_store.similarity_search(question)\n",
    "for d in docs:\n",
    "    print(f\"Page #{d.metadata['page']}  : {d.page_content[:100]}\")\n",
    "\n",
    "# Using MMR to diversify the results\n",
    "print(\"\\nMMR Search\")\n",
    "docs = vector_store.max_marginal_relevance_search(question,k=2, fetch_k=3)\n",
    "for d in docs:\n",
    "    print(f\"Page #{d.metadata['page']}  : {d.page_content[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some compression to avoid unnecessary text around the content we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y4/dbb_k7x92tq09002v_lhllnw0000gn/T/ipykernel_32138/2119036034.py:17: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  compressed_docs = compression_retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "There is a paper called “Chinchilla paper” that goes in deep to train LLMs optimally.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Muthu Arumugam\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
    "\n",
    "# Wrap our vectorstore\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vector_store.as_retriever(search_type=\"mmr\")\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generate\n",
    "Time to call ChatGPT for a response based on our retrieval. We will use Question & Answer to call LLM.\n",
    "\n",
    "#### Using Retrieval QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y4/dbb_k7x92tq09002v_lhllnw0000gn/T/ipykernel_32138/1721430867.py:11: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the author?\n",
      "Answer: The author of the articles on Generative AI projects is Muthu Arumugam.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vector_store.as_retriever()\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(f\"Question: {question}\\nAnswer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the author?\n",
      "Answer: The author of the Generative AI project articles is Muthu Arumugam. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(f\"Question: {question}\\nAnswer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modern LCEL Approach (Recommended)\n",
    "Using the new LangChain Expression Language (LCEL) for better composability and streaming support with Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Modern LCEL approach with streaming support\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in a concise manner. Always say \"thanks for asking!\" at the end of the answer.\"\"\")\n",
    "\n",
    "# Create the chain using LCEL with Pinecone vector store\n",
    "rag_chain = (\n",
    "    {\"context\": vector_store.as_retriever() | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Stream the response\n",
    "print(f\"Question: {question}\\nAnswer: \", end=\"\")\n",
    "for chunk in rag_chain.stream(question):\n",
    "    print(chunk, end=\"\")\n",
    "print()  # New line at the end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
