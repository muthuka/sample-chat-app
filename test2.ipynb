{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Demo\n",
    "Using RAG to power up an LLM. We will use Langchain for our example. Langchain framework makes build LLM apps super easy.\n",
    "\n",
    "![./flow.png](./flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Updated for LangChain 0.3.x (FAISS Version)\n",
    "\n",
    "This notebook demonstrates RAG with FAISS vector store using the latest LangChain APIs:\n",
    "\n",
    "- **Document Loaders**: `langchain-community.document_loaders`\n",
    "- **Text Splitters**: `langchain-text-splitters` package\n",
    "- **LLMs**: `langchain-openai` for OpenAI models\n",
    "- **Vector Store**: FAISS with updated method calls\n",
    "- **Chains**: Modern LCEL approach for better streaming and composability\n",
    "\n",
    "Both legacy and modern approaches are shown for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! pip3 install -q langchain pypdf openai faiss-cpu \\\n",
    "            langchain-openai chromadb langchain-community \\\n",
    "            langchain-text-splitters langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "### Step 1: Extract\n",
    "Load a document and extract the contents. For our example, I added a sample PDF from my article in docs folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages:  4\n",
      "First 100 chars of 2nd page:  dataset. Each model fits for few use cases.\n",
      "Autoen\n",
      "Metadata:  {'source': 'docs/GenAI-Part1.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"docs/GenAI-Part1.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "# Look into the doc\n",
    "second_page = pages[1]\n",
    "print(\"Total pages: \", len(pages))\n",
    "print(\"First 100 chars of 2nd page: \", second_page.page_content[:50])\n",
    "print(\"Metadata: \", second_page.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split\n",
    "Now split the document contents into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents on Charter Splitter:  6\n",
      "Total documents on Recursive Charter Splitter:  6\n",
      "Total documents on Token Splitter:  8\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Character Splitter\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    separator = '\\n',\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "c_docs = c_splitter.split_documents(pages)\n",
    "print(\"Total documents on Charter Splitter: \", len(c_docs))\n",
    "\n",
    "# Recursive Character Splitter\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "r_docs = r_splitter.split_documents(pages)\n",
    "print(\"Total documents on Recursive Charter Splitter: \", len(r_docs))\n",
    "\n",
    "# Token Splitter\n",
    "t_splitter = TokenTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "t_docs = t_splitter.split_documents(pages)\n",
    "print(\"Total documents on Token Splitter: \", len(t_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Vector Store\n",
    "Let's take our splits and embed them and then store them into a vector store. We will use [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss) which is an in-memory DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the pages of our PDF into Vector Store with Embeddings using OpenAI Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents on Vector Store:  6\n"
     ]
    }
   ],
   "source": [
    "vectordb = FAISS.from_documents(c_docs, OpenAIEmbeddings())\n",
    "print(\"Total documents on Vector Store: \", len(vectordb.docstore._dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Retrieve\n",
    "Let's retrieve with different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Search\n",
      "Page #1  : You can use quantization techniques to save memory by trading off the\n",
      "precision. Instead of a 32-bit\n",
      "Page #3  : Disclaimer: This is not generated by an AI bot. Also, a lot of these were\n",
      "learned through the DeepLe\n",
      "Page #2  : The measurement used to understand the training time is 1 “petaflop/s-day”\n",
      "= # floating point operat\n",
      "Page #3  : You can elect to choose a smaller model and then can train it for a specific\n",
      "field of yours. For exa\n",
      "\n",
      "MMR Search\n",
      "Page #1  : You can use quantization techniques to save memory by trading off the\n",
      "precision. Instead of a 32-bit\n",
      "Page #3  : Disclaimer: This is not generated by an AI bot. Also, a lot of these were\n",
      "learned through the DeepLe\n"
     ]
    }
   ],
   "source": [
    "question = \"What is DDP?\"\n",
    "\n",
    "# Using Similarity Search\n",
    "print(\"\\nSimilarity Search\")\n",
    "docs = vectordb.similarity_search(question, k=2)\n",
    "for d in docs:\n",
    "    print(f\"Page #{d.metadata['page']}  : {d.page_content[:100]}\")\n",
    "\n",
    "# Using MMR to diversify the results\n",
    "print(\"\\nMMR Search\")\n",
    "docs = vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)\n",
    "for d in docs:\n",
    "    print(f\"Page #{d.metadata['page']}  : {d.page_content[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some compression to avoid unnecessary text around the content we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y4/dbb_k7x92tq09002v_lhllnw0000gn/T/ipykernel_29310/3291913278.py:17: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  compressed_docs = compression_retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "DDP — Distributed Data Parallel\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Generative AI project — Part 1Muthu Arumugam\n",
      "This article will help you understand how you can get involved and create\n",
      "products using Generative AI models. To get a quick intro to Generative AI,\n",
      "look at my previous articles — Quickies.\n",
      "For an AI project, these are the following steps equivalent to SDLC.\n",
      "AI project lifecycle\n",
      "Use case discovery\n",
      "You have the option to choose from a variety of tasks from LLMs. You can\n",
      "choose 1 or many for your project from below:\n",
      "Essay Writing\n",
      "Summarization\n",
      "Translation from language to language\n",
      "Translation from language to code\n",
      "Information retrieval\n",
      "Call external APIs\n",
      "Model identification\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
    "\n",
    "# Wrap our vectorstore\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generate\n",
    "Time to call ChatGPT for a response based on our retrieval. We will use Question & Answer to call LLM.\n",
    "\n",
    "#### Using Retrieval QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is DDP?\n",
      "Answer: DDP stands for Distributed Data Parallel. It is a strategy used in deep learning to distribute the training of a model across multiple GPUs. This technique helps optimize the training process by dividing the workload among different GPUs, allowing for faster training times and efficient use of resources.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(f\"Question: {question}\\nAnswer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is DDP?\n",
      "Answer: DDP stands for Distributed Data Parallel, a strategy for splitting tasks into multiple GPUs to optimize training with expensive GPUs like NVIDIA A-100s. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(f\"Question: {question}\\nAnswer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modern LCEL Approach (Recommended)\n",
    "Using the new LangChain Expression Language (LCEL) for better composability and streaming support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Modern LCEL approach with streaming support\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in a concise manner. Always say \"thanks for asking!\" at the end of the answer.\"\"\")\n",
    "\n",
    "# Create the chain using LCEL\n",
    "rag_chain = (\n",
    "    {\"context\": vectordb.as_retriever() | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Stream the response\n",
    "print(f\"Question: {question}\\nAnswer: \", end=\"\")\n",
    "for chunk in rag_chain.stream(question):\n",
    "    print(chunk, end=\"\")\n",
    "print()  # New line at the end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
