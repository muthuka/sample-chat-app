{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Demo\n",
    "Using RAG to power up an LLM. We will use Langchain for our example. Langchain framework makes build LLM apps super easy.\n",
    "\n",
    "![./flow.png](./flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Updated for LangChain 0.3.x (FAISS Version)\n",
    "\n",
    "This notebook demonstrates RAG with FAISS vector store using the latest LangChain APIs:\n",
    "\n",
    "- **Document Loaders**: `langchain-community.document_loaders`\n",
    "- **Text Splitters**: `langchain-text-splitters` package\n",
    "- **LLMs**: `langchain-openai` for OpenAI models\n",
    "- **Vector Store**: FAISS with updated method calls\n",
    "- **Chains**: Modern LCEL approach for better streaming and composability\n",
    "\n",
    "Both legacy and modern approaches are shown for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "! pip3 install -q langchain pypdf openai faiss-cpu \\\n",
    "            langchain-openai chromadb langchain-community \\\n",
    "            langchain-text-splitters langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "### Step 1: Extract\n",
    "Load a document and extract the contents. For our example, I added a sample PDF from my article in docs folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"docs/GenAI-Part1.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "# Look into the doc\n",
    "second_page = pages[1]\n",
    "print(\"Total pages: \", len(pages))\n",
    "print(\"First 100 chars of 2nd page: \", second_page.page_content[:50])\n",
    "print(\"Metadata: \", second_page.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split\n",
    "Now split the document contents into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Character Splitter\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    separator = '\\n',\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "c_docs = c_splitter.split_documents(pages)\n",
    "print(\"Total documents on Charter Splitter: \", len(c_docs))\n",
    "\n",
    "# Recursive Character Splitter\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "r_docs = r_splitter.split_documents(pages)\n",
    "print(\"Total documents on Recursive Charter Splitter: \", len(r_docs))\n",
    "\n",
    "# Token Splitter\n",
    "t_splitter = TokenTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "t_docs = t_splitter.split_documents(pages)\n",
    "print(\"Total documents on Token Splitter: \", len(t_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Vector Store\n",
    "Let's take our splits and embed them and then store them into a vector store. We will use [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss) which is an in-memory DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the pages of our PDF into Vector Store with Embeddings using OpenAI Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = FAISS.from_documents(c_docs, OpenAIEmbeddings())\n",
    "print(\"Total documents on Vector Store: \", len(vectordb.docstore._dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Retrieve\n",
    "Let's retrieve with different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is DDP?\"\n",
    "\n",
    "# Using Similarity Search\n",
    "print(\"\\nSimilarity Search\")\n",
    "docs = vectordb.similarity_search(question, k=2)\n",
    "for d in docs:\n",
    "    print(f\"Page #{d.metadata['page']}  : {d.page_content[:100]}\")\n",
    "\n",
    "# Using MMR to diversify the results\n",
    "print(\"\\nMMR Search\")\n",
    "docs = vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)\n",
    "for d in docs:\n",
    "    print(f\"Page #{d.metadata['page']}  : {d.page_content[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some compression to avoid unnecessary text around the content we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
    "\n",
    "# Wrap our vectorstore\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generate\n",
    "Time to call ChatGPT for a response based on our retrieval. We will use Question & Answer to call LLM.\n",
    "\n",
    "#### Using Retrieval QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(f\"Question: {question}\\nAnswer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(f\"Question: {question}\\nAnswer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modern LCEL Approach (Recommended)\n",
    "Using the new LangChain Expression Language (LCEL) for better composability and streaming support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Modern LCEL approach with streaming support\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in a concise manner. Always say \"thanks for asking!\" at the end of the answer.\"\"\")\n",
    "\n",
    "# Create the chain using LCEL\n",
    "rag_chain = (\n",
    "    {\"context\": vectordb.as_retriever() | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Stream the response\n",
    "print(f\"Question: {question}\\nAnswer: \", end=\"\")\n",
    "for chunk in rag_chain.stream(question):\n",
    "    print(chunk, end=\"\")\n",
    "print()  # New line at the end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
