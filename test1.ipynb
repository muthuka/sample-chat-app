{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Demo\n",
    "Using RAG to power up an LLM. We will use Langchain for our example. Langchain framework makes build LLM apps super easy.\n",
    "\n",
    "![./flow.png](./flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Updated for LangChain 0.3.x\n",
    "\n",
    "This notebook has been migrated to use the latest LangChain APIs:\n",
    "\n",
    "- **Document Loaders**: Now using `langchain-community`\n",
    "- **Text Splitters**: Updated to `langchain-text-splitters` \n",
    "- **LLMs**: Migrated to `langchain-openai`\n",
    "- **Retrievers**: Updated to new API methods\n",
    "- **Chains**: Added modern LCEL approach alongside legacy chains\n",
    "- **Method calls**: Updated from `()` to `.invoke()` for consistency\n",
    "\n",
    "The legacy approach is maintained for educational purposes, with modern LCEL examples added for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q chromadb pypdf tqdm tiktoken \\\n",
    "        langchain langchain-openai langchain-chroma langchain-community \\\n",
    "        langchain-huggingface langchain-text-splitters langchain-core \\\n",
    "        sentence_transformers\n",
    "        # Additional packages for modern features: sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "### Step 1\n",
    "Load a document and extract the contents. For our example, I added a sample PDF from my article in docs folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muthuka/root/muthuka/sample-chat-app/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages:  4\n",
      "First 100 chars of 2nd page:  dataset. Each model fits for few use cases.\n",
      "Autoen\n",
      "Metadata:  {'producer': 'macOS Version 14.2 (Build 23C5055b) Quartz PDFContext', 'creator': 'Safari', 'creationdate': \"D:20231201164323Z00'00'\", 'title': 'Generative AI project — Part 1', 'author': 'Muthu Arumugam', 'subject': 'Generative AI project', 'moddate': \"D:20231201164323Z00'00'\", 'keywords': 'AI, LLM', 'aapl:keywords': \"['AI', 'LLM']\", 'source': 'docs/GenAI-Part1.pdf', 'total_pages': 4, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"docs/GenAI-Part1.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "# Look into the doc\n",
    "second_page = pages[1]\n",
    "print(\"Total pages: \", len(pages))\n",
    "print(\"First 100 chars of 2nd page: \", second_page.page_content[:50])\n",
    "print(\"Metadata: \", second_page.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Now split the document contents into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents on Charter Splitter:  6\n",
      "Total documents on Recursive Charter Splitter:  6\n",
      "Total documents on Token Splitter:  8\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Character Splitter\n",
    "c_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    separator = '\\n',\n",
    "    length_function = len\n",
    ")\n",
    "\n",
    "c_docs = c_splitter.split_documents(pages)\n",
    "print(\"Total documents on Charter Splitter: \", len(c_docs))\n",
    "\n",
    "# Recursive Character Splitter\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150, \n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "r_docs = r_splitter.split_documents(pages)\n",
    "print(\"Total documents on Recursive Charter Splitter: \", len(r_docs))\n",
    "\n",
    "# Token Splitter\n",
    "t_splitter = TokenTextSplitter(\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "t_docs = t_splitter.split_documents(pages)\n",
    "print(\"Total documents on Token Splitter: \", len(t_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "Let's take our splits and embed them and then store them into a vector store. We will use [ChromaDB](https://python.langchain.com/docs/integrations/vectorstores/chroma) which is an in-memory DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "persist_directory = 'docs/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf ./docs/chroma  # remove old database files if any\n",
    "!mkdir -p ./docs/chroma  # create a directory to store the database files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the pages of our PDF into Vector Store with Embeddings using OpenAI Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y4/dbb_k7x92tq09002v_lhllnw0000gn/T/ipykernel_39363/2136938274.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents on Vector Store:  4\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectordb = Chroma.from_documents(pages, embedding_function)\n",
    "\n",
    "print(\"Total documents on Vector Store: \", vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do some search and see some relevant content in this vector db."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "Let's retrieve with different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Search\n",
      "Page #2  : The measurement used to understand the training time is 1 “petaflop/s-day”\n",
      "= # floating point operat\n",
      "Page #0  : Generative AI project — Part 1Muthu Arumugam\n",
      "This article will help you understand how you can get i\n",
      "Page #3  : You can elect to choose a smaller model and then can train it for a specific\n",
      "field of yours. For exa\n",
      "Page #1  : dataset. Each model fits for few use cases.\n",
      "Autoencoding models — BERT/ROBERTA — Sentiment analysis,\n",
      "\n",
      "MMR Search\n",
      "Page #2  : The measurement used to understand the training time is 1 “petaflop/s-day”\n",
      "= # floating point operat\n",
      "Page #0  : Generative AI project — Part 1Muthu Arumugam\n",
      "This article will help you understand how you can get i\n"
     ]
    }
   ],
   "source": [
    "question = \"What is DDP?\"\n",
    "\n",
    "# Using Similarity Search\n",
    "print(\"\\nSimilarity Search\")\n",
    "docs = vectordb.similarity_search_with_score(question)\n",
    "for d in docs:\n",
    "    print(f\"Page #{d[0].metadata['page']}  : {d[0].page_content[:100]}\")\n",
    "\n",
    "# Using MMR to diversify the results\n",
    "print(\"\\nMMR Search\")\n",
    "docs = vectordb.max_marginal_relevance_search(question,k=2, fetch_k=3)\n",
    "for d in docs:\n",
    "    print(f\"Page #{d.metadata['page']}  : {d.page_content[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some compression to avoid unnecessary text around the content we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "There is a paper called “Chinchilla paper” that goes in deep to train LLMs\n",
      "optimally. Also, you have to consider the size of the training data model\n",
      "which is the ideal size of ~20x. If the model uses 70B parameters, you need\n",
      "to feed ~1.4T tokens of a dataset.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Generative AI project — Part 1Muthu Arumugam\n",
      "This article will help you understand how you can get involved and create\n",
      "products using Generative AI models. To get a quick intro to Generative AI,\n",
      "look at my previous articles — Quickies.\n",
      "For an AI project, these are the following steps equivalent to SDLC.\n",
      "AI project lifecycle\n",
      "Use case discovery\n",
      "You have the option to choose from a variety of tasks from LLMs. You can\n",
      "choose 1 or many for your project from below:\n",
      "Essay Writing\n",
      "Summarization\n",
      "Translation from language to language\n",
      "Translation from language to code\n",
      "Information retrieval\n",
      "Call external APIs\n",
      "Model identification\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "- Bloomberg published a model which is trained for finance-related LLM.\n",
      "- Introducing BloombergGPT, Bloomberg’s 50-billion parameter large language model, purpose-built from scratch for finance | Press | Bloomberg LP\n",
      "- Look at Hugging Face to understand with a model card of their own.\n",
      "- Hugging Face Model hub — https://huggingface.co/models\n",
      "- Hugging Face Tasks — https://huggingface.co/tasks\n",
      "- The title image was generated by DALL-E through the Bing chatbot.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "DDP — Distributed Data Parallel\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
    "\n",
    "# Wrap our vectorstore\n",
    "llm = OpenAI(temperature=0)\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "Time to call ChatGPT for a response based on our retrieval. We will use Question & Answer to call LLM.\n",
    "\n",
    "#### Using Retrieval QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is DDP?\n",
      "Answer: DDP stands for Distributed Data Parallel. It is a strategy used in training deep learning models that involves distributing the data across multiple GPUs to parallelize the training process. This helps optimize the training process and reduce the time required to train large models.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(f\"Question: {question}\\nAnswer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is DDP?\n",
      "Answer: DDP stands for Distributed Data Parallel, a strategy for splitting tasks into multiple GPUs to optimize training models. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "print(f\"Question: {question}\\nAnswer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modern LCEL Approach (Recommended)\n",
    "Using the new LangChain Expression Language (LCEL) for better composability and streaming support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is DDP?\n",
      "Answer: DDP stands for Distributed Data Parallel. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Modern LCEL approach with streaming support\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in a concise manner. Always say \"thanks for asking!\" at the end of the answer.\"\"\")\n",
    "\n",
    "# Create the chain using LCEL\n",
    "rag_chain = (\n",
    "    {\"context\": vectordb.as_retriever() | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Stream the response\n",
    "print(f\"Question: {question}\\nAnswer: \", end=\"\")\n",
    "for chunk in rag_chain.stream(question):\n",
    "    print(chunk, end=\"\")\n",
    "print()  # New line at the end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
